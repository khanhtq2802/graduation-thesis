{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "import random\n",
    "import string\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, DataCollatorWithPadding, AdamW, get_scheduler, MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"../data/raw_dataset/train.csv\",\n",
    "    \"test\": \"../data/raw_dataset/test.csv\"\n",
    "}\n",
    "raw_dataset = load_dataset(\"csv\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tang cuong va can bang du lieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng thiết bị (GPU nếu có sẵn, nếu không thì sử dụng CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Danh sách các ngôn ngữ\n",
    "languages = ['de', 'es', 'ru', 'fr']\n",
    "translate_models = {}\n",
    "translate_tokenizers = {}\n",
    "# Tải mô hình và tokenizer, sau đó chuyển mô hình sang thiết bị\n",
    "for language in languages:\n",
    "    translate_models['to' + language] = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-\" + language).to(device)\n",
    "    translate_tokenizers['to' + language] = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-\" + language)\n",
    "    translate_models['from' + language] = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-\" + language + \"-en\").to(device)\n",
    "    translate_tokenizers['from' + language] = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-\" + language + \"-en\")\n",
    "languages.append('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm back-translate\n",
    "def back_translate(texts):\n",
    "    language = random.choice(languages)\n",
    "    if language == 'en':\n",
    "        return texts\n",
    "    # Dịch từ English sang ngôn ngữ đích\n",
    "    model = translate_models['to' + language]\n",
    "    tokenizer = translate_tokenizers['to' + language]\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        translated = model.generate(**inputs)\n",
    "    texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    # Giải phóng bộ nhớ GPU cho model và inputs\n",
    "    del inputs\n",
    "    del translated\n",
    "    torch.cuda.empty_cache()\n",
    "    # Dịch từ ngôn ngữ đích quay lại English\n",
    "    model = translate_models['from' + language]\n",
    "    tokenizer = translate_tokenizers['from' + language]\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        translated = model.generate(**inputs)\n",
    "    texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    # Giải phóng bộ nhớ GPU cho model và inputs\n",
    "    del inputs\n",
    "    del translated\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del language\n",
    "    gc.collect()\n",
    "    return texts\n",
    "def remove_last_punctuation(text):\n",
    "    while text[-1] in string.punctuation:\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "def create_train_dataset(raw_datasets, batch_size=48, use_back_translate=True):\n",
    "    datasets = DatasetDict({'train': raw_datasets['train'].shuffle()})\n",
    "    if use_back_translate:\n",
    "        en_text = datasets['train']['en_text']\n",
    "        new_en_text = []\n",
    "        for i in range(0, len(en_text), batch_size):\n",
    "            new_en_text.extend(back_translate(en_text[i : i + batch_size]))\n",
    "        datasets['train'] = datasets['train'].remove_columns('en_text').add_column('en_text', new_en_text)\n",
    "    #################################################################################################\n",
    "    for name in datasets:\n",
    "        data_dict = {\"en_text\": [], \"labels\": [], \"type\": []}\n",
    "        for row in datasets[name]:\n",
    "            text = row['en_text']\n",
    "            label_aspects = {'all': 0, 'amn': 0, 'ch': 0, 'ppl': 0, 'mgt': 0, 'nat': 0,}\n",
    "            labels = row['labels'].split()\n",
    "            for label in labels:\n",
    "                try:\n",
    "                    key, value = label.split('-')\n",
    "                except:\n",
    "                    print(\"Unknown label with text:\" + text)\n",
    "                if(key not in label_aspects or value not in ['0', '1', '2', '3']):\n",
    "                    raise Exception(\"Unknown label:\", label)\n",
    "                label_aspects[key] = int(value)\n",
    "            data_dict[\"en_text\"].append(text); data_dict[\"labels\"].append(label_aspects['all']);  data_dict[\"type\"].append('all-' + str(label_aspects['all']))\n",
    "            data_dict[\"en_text\"].append(text); data_dict[\"labels\"].append(label_aspects['amn']);  data_dict[\"type\"].append('amn-' + str(label_aspects['amn']))\n",
    "            data_dict[\"en_text\"].append(text); data_dict[\"labels\"].append(label_aspects['ch']);   data_dict[\"type\"].append('ch-' +  str(label_aspects['ch']))\n",
    "            data_dict[\"en_text\"].append(text); data_dict[\"labels\"].append(label_aspects['ppl']);  data_dict[\"type\"].append('ppl-' + str(label_aspects['ppl']))\n",
    "            data_dict[\"en_text\"].append(text); data_dict[\"labels\"].append(label_aspects['mgt']);  data_dict[\"type\"].append('mgt-' + str(label_aspects['mgt']))\n",
    "            data_dict[\"en_text\"].append(text); data_dict[\"labels\"].append(label_aspects['nat']);  data_dict[\"type\"].append('nat-' + str(label_aspects['nat']))\n",
    "        datasets[name] = Dataset.from_dict(DatasetDict(data_dict))\n",
    "    #################################################################################################\n",
    "    new_datasets = {\"en_text\": [], \"labels\": [], \"type\": []}\n",
    "    Max = 0\n",
    "    aspect_categories = ['all', 'amn', 'ch', 'ppl', 'mgt', 'nat']\n",
    "    positive_short2full = {\n",
    "        'all': 'positive all: ',\n",
    "        'amn': 'positive amenities: ',\n",
    "        'ch':  'positive cultural heritage: ',\n",
    "        'ppl': 'positive people: ',\n",
    "        'mgt': 'positive management: ',\n",
    "        'nat': 'positive nature: ',\n",
    "    }\n",
    "    negative_short2full = {\n",
    "        'all': 'negative all: ',\n",
    "        'amn': 'negative amenities: ',\n",
    "        'ch':  'negative cultural heritage: ',\n",
    "        'ppl': 'negative people: ',\n",
    "        'mgt': 'negative management: ',\n",
    "        'nat': 'negative nature: ',\n",
    "    }\n",
    "    # 2 cau hoa positive-*-1#################################################################################################\n",
    "    for aspect_category in aspect_categories:\n",
    "        type = aspect_category + '-1'\n",
    "        polarity_1_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        for i in range(polarity_1_datasets.num_rows):\n",
    "            # 1 + any\n",
    "            first_sentence = polarity_1_datasets[i][\"en_text\"]\n",
    "            second_sentence = datasets['train'][random.randint(0, datasets['train'].num_rows - 1)][\"en_text\"]\n",
    "            if random.randint(0, 1):\n",
    "                new_datasets['en_text'].append(positive_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('positive-' + aspect_category + '-1')\n",
    "            else:\n",
    "                new_datasets['en_text'].append(positive_short2full[aspect_category] + remove_last_punctuation(second_sentence) + \", \" + first_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('positive-' + aspect_category + '-1')\n",
    "        type = aspect_category + '-3'\n",
    "        polarity_3_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        for i in range(polarity_3_datasets.num_rows):\n",
    "            # 3 + any\n",
    "            first_sentence = polarity_3_datasets[i][\"en_text\"]\n",
    "            second_sentence = datasets['train'][random.randint(0, datasets['train'].num_rows - 1)][\"en_text\"]\n",
    "            if random.randint(0, 1):\n",
    "                new_datasets['en_text'].append(positive_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('positive-' + aspect_category + '-1')\n",
    "            else:\n",
    "                new_datasets['en_text'].append(positive_short2full[aspect_category] + remove_last_punctuation(second_sentence) + \", \" + first_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('positive-' + aspect_category + '-1')\n",
    "        Max = max(Max, polarity_1_datasets.num_rows + polarity_3_datasets.num_rows)\n",
    "    # 2 cau hoa negative-*-1#################################################################################################\n",
    "    for aspect_category in aspect_categories:\n",
    "        type = aspect_category + '-2'\n",
    "        polarity_2_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        for i in range(polarity_2_datasets.num_rows):\n",
    "            # 2 + any\n",
    "            first_sentence = polarity_2_datasets[i][\"en_text\"]\n",
    "            second_sentence = datasets['train'][random.randint(0, datasets['train'].num_rows - 1)][\"en_text\"]\n",
    "            if random.randint(0, 1):\n",
    "                new_datasets['en_text'].append(negative_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('negative-' + aspect_category + '-1')\n",
    "            else:\n",
    "                new_datasets['en_text'].append(negative_short2full[aspect_category] + remove_last_punctuation(second_sentence) + \", \" + first_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('negative-' + aspect_category + '-1')\n",
    "        type = aspect_category + '-3'\n",
    "        polarity_3_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        for i in range(polarity_3_datasets.num_rows):\n",
    "            # 3 + any\n",
    "            first_sentence = polarity_3_datasets[i][\"en_text\"]\n",
    "            second_sentence = datasets['train'][random.randint(0, datasets['train'].num_rows - 1)][\"en_text\"]\n",
    "            if random.randint(0, 1):\n",
    "                new_datasets['en_text'].append(negative_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('negative-' + aspect_category + '-1')\n",
    "            else:\n",
    "                new_datasets['en_text'].append(negative_short2full[aspect_category] + remove_last_punctuation(second_sentence) + \", \" + first_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('negative-' + aspect_category + '-1')\n",
    "        Max = max(Max, polarity_1_datasets.num_rows + polarity_3_datasets.num_rows)\n",
    "    # Can bang du lieu positive-*-1#################################################################################################\n",
    "    for aspect_category in aspect_categories:\n",
    "        type = aspect_category + '-1'\n",
    "        polarity_1_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_1 = polarity_1_datasets.num_rows\n",
    "\n",
    "        type = aspect_category + '-3'\n",
    "        polarity_3_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_3 = polarity_3_datasets.num_rows\n",
    "        \n",
    "        probability = num_rows_1/(num_rows_1 + num_rows_3)\n",
    "        for i in range(num_rows_1 + num_rows_3, Max):\n",
    "            first_sentence = ''\n",
    "            second_sentence = datasets['train'][random.randint(0, datasets['train'].num_rows - 1)][\"en_text\"]\n",
    "            if (random.random() < probability):\n",
    "                first_sentence = polarity_1_datasets[random.randint(0, num_rows_1 - 1)][\"en_text\"]\n",
    "            else:\n",
    "                first_sentence = polarity_3_datasets[random.randint(0, num_rows_3 - 1)][\"en_text\"]\n",
    "            if random.randint(0, 1):\n",
    "                new_datasets['en_text'].append(positive_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('positive-' + aspect_category + '-1')\n",
    "            else:\n",
    "                new_datasets['en_text'].append(positive_short2full[aspect_category] + remove_last_punctuation(second_sentence) + \", \" + first_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('positive-' + aspect_category + '-1')\n",
    "    # Can bang du lieu negative-*-1#################################################################################################\n",
    "    for aspect_category in aspect_categories:\n",
    "        type = aspect_category + '-2'\n",
    "        polarity_2_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_2 = polarity_2_datasets.num_rows\n",
    "\n",
    "        type = aspect_category + '-3'\n",
    "        polarity_3_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_3 = polarity_3_datasets.num_rows\n",
    "        \n",
    "        probability = num_rows_2/(num_rows_2 + num_rows_3)\n",
    "        for i in range(num_rows_2 + num_rows_3, Max):\n",
    "            first_sentence = ''\n",
    "            second_sentence = datasets['train'][random.randint(0, datasets['train'].num_rows - 1)][\"en_text\"]\n",
    "            if (random.random() < probability):\n",
    "                first_sentence = polarity_2_datasets[random.randint(0, num_rows_2 - 1)][\"en_text\"]\n",
    "            else:\n",
    "                first_sentence = polarity_3_datasets[random.randint(0, num_rows_3 - 1)][\"en_text\"]\n",
    "            if random.randint(0, 1):\n",
    "                new_datasets['en_text'].append(negative_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('negative-' + aspect_category + '-1')\n",
    "            else:\n",
    "                new_datasets['en_text'].append(negative_short2full[aspect_category] + remove_last_punctuation(second_sentence) + \", \" + first_sentence)\n",
    "                new_datasets['labels'].append(1)\n",
    "                new_datasets['type'].append('negative-' + aspect_category + '-1')\n",
    "    # Tao du lieu positive-*-0#################################################################################################\n",
    "    for aspect_category in aspect_categories:\n",
    "        type = aspect_category + '-0'\n",
    "        polarity_0_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_0 = polarity_0_datasets.num_rows\n",
    "\n",
    "        type = aspect_category + '-2'\n",
    "        polarity_2_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_2 = polarity_2_datasets.num_rows\n",
    "        for i in range(Max):\n",
    "            first_sentence = ''\n",
    "            if random.randint(0, 1):\n",
    "                first_sentence = polarity_0_datasets[random.randint(0, num_rows_0 - 1)][\"en_text\"]\n",
    "            else:\n",
    "                first_sentence = polarity_2_datasets[random.randint(0, num_rows_2 - 1)][\"en_text\"]\n",
    "\n",
    "            second_sentence = ''\n",
    "            if random.randint(0, 1):\n",
    "                second_sentence = polarity_0_datasets[random.randint(0, num_rows_0 - 1)][\"en_text\"]\n",
    "            else:\n",
    "                second_sentence = polarity_2_datasets[random.randint(0, num_rows_2 - 1)][\"en_text\"]\n",
    "            \n",
    "            new_datasets['en_text'].append(positive_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "            new_datasets['labels'].append(0)\n",
    "            new_datasets['type'].append('positive-' + aspect_category + '-0')\n",
    "    # Tao du lieu negative-*-0#################################################################################################\n",
    "    for aspect_category in aspect_categories:\n",
    "        type = aspect_category + '-0'\n",
    "        polarity_0_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_0 = polarity_0_datasets.num_rows\n",
    "\n",
    "        type = aspect_category + '-1'\n",
    "        polarity_1_datasets = datasets['train'].filter(lambda x: x['type'] == type)\n",
    "        num_rows_1 = polarity_1_datasets.num_rows\n",
    "        for i in range(Max):\n",
    "            first_sentence = ''\n",
    "            if random.randint(0, 1):\n",
    "                first_sentence = polarity_0_datasets[random.randint(0, num_rows_0 - 1)][\"en_text\"]\n",
    "            else:\n",
    "                first_sentence = polarity_1_datasets[random.randint(0, num_rows_1 - 1)][\"en_text\"]\n",
    "\n",
    "            second_sentence = ''\n",
    "            if random.randint(0, 1):\n",
    "                second_sentence = polarity_0_datasets[random.randint(0, num_rows_0 - 1)][\"en_text\"]\n",
    "            else:\n",
    "                second_sentence = polarity_1_datasets[random.randint(0, num_rows_1 - 1)][\"en_text\"]\n",
    "            \n",
    "            new_datasets['en_text'].append(negative_short2full[aspect_category] + remove_last_punctuation(first_sentence) + \", \" + second_sentence)\n",
    "            new_datasets['labels'].append(0)\n",
    "            new_datasets['type'].append('negative-' + aspect_category + '-0')\n",
    "    clear_output()\n",
    "    return DatasetDict({\"train\": Dataset.from_dict(DatasetDict(new_datasets))})\n",
    "\n",
    "def polarity2label(check_polarity, polarity):\n",
    "    if check_polarity == 1:\n",
    "        if polarity == 1 or polarity == 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    if check_polarity == 2:\n",
    "        if polarity == 2 or polarity == 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def create_test_dataset(raw_datasets):\n",
    "    datasets = DatasetDict({\n",
    "        'train': raw_datasets['train'],\n",
    "        'test': raw_datasets['test']})\n",
    "    for name in datasets:\n",
    "        data_dict = {\"en_text\": [], \"labels\": [], \"type\": []}\n",
    "        for item in datasets[name]:\n",
    "            text = item['en_text']\n",
    "            aspect2label = {'all': 0, 'amn': 0, 'ch': 0, 'ppl': 0, 'mgt': 0, 'nat': 0,}\n",
    "            labels = item['labels'].split()\n",
    "            for label in labels:\n",
    "                try:\n",
    "                    key, value = label.split('-')\n",
    "                except:\n",
    "                    print(\"Unknown label with text:\" + text)\n",
    "                if(key not in aspect2label or value not in ['0', '1', '2', '3']):\n",
    "                    raise Exception(\"Unknown label:\", label)\n",
    "                aspect2label[key] = int(value)\n",
    "            data_dict[\"en_text\"].append(\"positive all: \" + text);              data_dict[\"labels\"].append(polarity2label(1, aspect2label['all']));data_dict[\"type\"].append('positive-all-' + str(polarity2label(1, aspect2label['all'])))\n",
    "            data_dict[\"en_text\"].append(\"positive amenities: \" + text);        data_dict[\"labels\"].append(polarity2label(1, aspect2label['amn']));data_dict[\"type\"].append('positive-amn-' + str(polarity2label(1, aspect2label['amn'])))\n",
    "            data_dict[\"en_text\"].append(\"positive cultural heritage: \" + text);data_dict[\"labels\"].append(polarity2label(1, aspect2label['ch'])); data_dict[\"type\"].append('positive-ch-'  + str(polarity2label(1, aspect2label['ch'])))\n",
    "            data_dict[\"en_text\"].append(\"positive people: \" + text);           data_dict[\"labels\"].append(polarity2label(1, aspect2label['ppl']));data_dict[\"type\"].append('positive-ppl-' + str(polarity2label(1, aspect2label['ppl'])))\n",
    "            data_dict[\"en_text\"].append(\"positive management: \" + text);       data_dict[\"labels\"].append(polarity2label(1, aspect2label['mgt']));data_dict[\"type\"].append('positive-mgt-' + str(polarity2label(1, aspect2label['mgt'])))\n",
    "            data_dict[\"en_text\"].append(\"positive nature: \" + text);           data_dict[\"labels\"].append(polarity2label(1, aspect2label['nat']));data_dict[\"type\"].append('positive-nat-' + str(polarity2label(1, aspect2label['nat'])))\n",
    "\n",
    "            data_dict[\"en_text\"].append(\"negative all: \" + text);              data_dict[\"labels\"].append(polarity2label(2, aspect2label['all']));data_dict[\"type\"].append('negative-all-' + str(polarity2label(2, aspect2label['all'])))\n",
    "            data_dict[\"en_text\"].append(\"negative amenities: \" + text);        data_dict[\"labels\"].append(polarity2label(2, aspect2label['amn']));data_dict[\"type\"].append('negative-amn-' + str(polarity2label(2, aspect2label['amn'])))\n",
    "            data_dict[\"en_text\"].append(\"negative cultural heritage: \" + text);data_dict[\"labels\"].append(polarity2label(2, aspect2label['ch'])); data_dict[\"type\"].append('negative-ch-'  + str(polarity2label(2, aspect2label['ch'])))\n",
    "            data_dict[\"en_text\"].append(\"negative people: \" + text);           data_dict[\"labels\"].append(polarity2label(2, aspect2label['ppl']));data_dict[\"type\"].append('negative-ppl-' + str(polarity2label(2, aspect2label['ppl'])))\n",
    "            data_dict[\"en_text\"].append(\"negative management: \" + text);       data_dict[\"labels\"].append(polarity2label(2, aspect2label['mgt']));data_dict[\"type\"].append('negative-mgt-' + str(polarity2label(2, aspect2label['mgt'])))\n",
    "            data_dict[\"en_text\"].append(\"negative nature: \" + text);           data_dict[\"labels\"].append(polarity2label(2, aspect2label['nat']));data_dict[\"type\"].append('negative-nat-' + str(polarity2label(2, aspect2label['nat'])))\n",
    "        datasets[name] = Dataset.from_dict(DatasetDict(data_dict))\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, set freeze, set device\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"khanhtq2802/thesis-model\")\n",
    "config = AutoConfig.from_pretrained(\"khanhtq2802/thesis-model\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../best_weights\", num_labels=2, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# unFreeze\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"classifier.out_proj\"):\n",
    "        param.requires_grad = True\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"classifier.dense\"):\n",
    "        param.requires_grad = True\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"roberta.encoder.layer.11\"):\n",
    "        param.requires_grad = True\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable Parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách các checkpolarity-aspect\n",
    "aspects = [\"positive all: \", \"positive amenities: \", \"positive cultural heritage: \", \"positive people: \", \"positive management: \", \"positive nature: \",\n",
    "           \"negative all: \", \"negative amenities: \", \"negative cultural heritage: \", \"negative people: \", \"negative management: \", \"negative nature: \"]\n",
    "skip_tokens = 0\n",
    "# Chuyển các aspect sang token và đếm số lượng token\n",
    "for aspect in aspects:\n",
    "    tokens = tokenizer.tokenize(aspect)\n",
    "    skip_tokens = max(skip_tokens, len(tokens))\n",
    "print(skip_tokens)\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15, skip_tokens=4):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.skip_tokens = skip_tokens\n",
    "    def __call__(self, examples):\n",
    "        input_ids = [example['input_ids'] for example in examples]\n",
    "        attention_mask = [example['attention_mask'] for example in examples]\n",
    "        labels = [example['labels'] for example in examples]\n",
    "        # Mask tokens with probability self.mlm_probability\n",
    "        for i in range(len(input_ids)):\n",
    "            for j in range(self.skip_tokens + 1, len(input_ids[i])):\n",
    "                if random.random() < self.mlm_probability:\n",
    "                    input_ids[i][j] = self.tokenizer.mask_token_id\n",
    "        # Convert input_ids to tensor\n",
    "        input_ids = pad_sequence([torch.tensor(sublist) for sublist in input_ids], batch_first=True, padding_value=1)\n",
    "        # Pad attention_mask and convert to tensor\n",
    "        attention_mask = pad_sequence([torch.tensor(sublist) for sublist in attention_mask], batch_first=True, padding_value=1)\n",
    "        return {\n",
    "            'labels': torch.tensor(labels),\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,}\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"en_text\"], truncation=True)\n",
    "data_collator_train = CustomDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,  # 15% masking rate\n",
    "    skip_tokens=skip_tokens\n",
    ")\n",
    "data_collator_test = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load loss history from file\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "try:\n",
    "    with open(\"losses_2.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Check if line is not empty\n",
    "                training_loss, test_loss = line.split(\",\")\n",
    "                training_losses.append(float(training_loss))\n",
    "                test_losses.append(float(test_loss))\n",
    "except:\n",
    "    print(\"error when load training history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    create_test_dataset(raw_dataset).map(tokenize_function, batched=True).remove_columns(['en_text', 'type'])['test'], \n",
    "    shuffle=False, \n",
    "    batch_size=24, \n",
    "    collate_fn=data_collator_test)\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(\n",
    "    create_train_dataset(raw_dataset, use_back_translate=True).map(tokenize_function, batched=True).remove_columns([\"en_text\", \"type\"])[\"train\"], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator_train)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5) #before 1e-5\n",
    "num_epochs = 100\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "decay = \"cosine\" #constant cosine linear\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    decay,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=len(train_dataloader),\n",
    "    num_training_steps=num_training_steps)\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    training_losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "    # evaluation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            total_loss += torch.nn.CrossEntropyLoss()(model(**{k: v.to(device) for k, v in batch.items() if k != 'labels'}).logits, batch['labels'].to(device)).item()\n",
    "    test_losses.append(total_loss / len(test_dataloader))\n",
    "    # Plotting\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(training_losses, label=\"Training Loss\")\n",
    "    plt.plot(test_losses, label=\"test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # save loss history to file\n",
    "    with open(\"losses.txt\", \"w\") as f:\n",
    "        for i in range(len(training_losses)):\n",
    "            f.write(f\"{training_losses[i]},{test_losses[i]}\\n\")\n",
    "    # save model\n",
    "    model.save_pretrained(\"../late_weights\")\n",
    "    if test_losses[-1] == min(test_losses):\n",
    "        model.save_pretrained(\"../best_weights\")\n",
    "    # re-create train_dataloader\n",
    "    if len(test_losses) >= 2:\n",
    "        if test_losses[-1] >= test_losses[-2]:\n",
    "            torch.cuda.empty_cache()\n",
    "            train_dataloader = DataLoader(\n",
    "                create_train_dataset(raw_dataset, use_back_translate=True).map(tokenize_function, batched=True).remove_columns([\"en_text\", \"type\"])[\"train\"], \n",
    "                shuffle=True,\n",
    "                batch_size=batch_size, \n",
    "                collate_fn=data_collator_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, raw_datasets, device, name='train'):\n",
    "    dataset = create_test_dataset(raw_datasets)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "    dataset.map(tokenize_function, batched=True).remove_columns(['en_text', 'type'])[name], \n",
    "    shuffle=False,\n",
    "    batch_size=16, \n",
    "    collate_fn=data_collator_test)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_acc, total_loss = 0, 0\n",
    "    fail_count = {\n",
    "        \"false-positive\": {\"all-0\": 0, \"amn-0\": 0, \"ch-0\": 0, \"hist-0\": 0, \"ppl-0\": 0, \"mgt-0\": 0, \"nat-0\": 0,\n",
    "        \"all-1\": 0, \"amn-1\": 0, \"ch-1\": 0, \"hist-1\": 0, \"ppl-1\": 0, \"mgt-1\": 0, \"nat-1\": 0,\n",
    "        \"all-2\": 0, \"amn-2\": 0, \"ch-2\": 0, \"hist-2\": 0, \"ppl-2\": 0, \"mgt-2\": 0, \"nat-2\": 0,\n",
    "        \"all-3\": 0, \"amn-3\": 0, \"ch-3\": 0, \"hist-3\": 0, \"ppl-3\": 0, \"mgt-3\": 0, \"nat-3\": 0,},\n",
    "\n",
    "        \"false-negative\": {\"all-0\": 0, \"amn-0\": 0, \"ch-0\": 0, \"hist-0\": 0, \"ppl-0\": 0, \"mgt-0\": 0, \"nat-0\": 0,\n",
    "        \"all-1\": 0, \"amn-1\": 0, \"ch-1\": 0, \"hist-1\": 0, \"ppl-1\": 0, \"mgt-1\": 0, \"nat-1\": 0,\n",
    "        \"all-2\": 0, \"amn-2\": 0, \"ch-2\": 0, \"hist-2\": 0, \"ppl-2\": 0, \"mgt-2\": 0, \"nat-2\": 0,\n",
    "        \"all-3\": 0, \"amn-3\": 0, \"ch-3\": 0, \"hist-3\": 0, \"ppl-3\": 0, \"mgt-3\": 0, \"nat-3\": 0,},\n",
    "    }\n",
    "    type_counts = {}\n",
    "    for type_ in dataset[name]['type']:\n",
    "        if type_ in type_counts:\n",
    "            type_counts[type_] += 1\n",
    "        else:\n",
    "            type_counts[type_] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move data to the specified device\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits  # Access the model's output logits\n",
    "\n",
    "            # Calculate loss (optional, for reference)\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            pred_labels = torch.argmax(logits, dim=1)  # Get predicted labels\n",
    "            match_labels = pred_labels == labels\n",
    "            for i in range(len(match_labels)):\n",
    "                if match_labels[i] == True:\n",
    "                    total_acc+=1\n",
    "                else:\n",
    "                    text = tokenizer.decode(batch['input_ids'][i])\n",
    "                    if text.startswith(\"<s>all: \"):\n",
    "                        fail_count[\"false-positive\"][\"all-\" + str(pred_labels[i].item())] += 1\n",
    "                        fail_count[\"false-negative\"][\"all-\" + str(labels[i].item())] += 1\n",
    "                    if text.startswith(\"<s>amenities: \"):\n",
    "                        fail_count[\"false-positive\"][\"amn-\" + str(pred_labels[i].item())] += 1\n",
    "                        fail_count[\"false-negative\"][\"amn-\" + str(labels[i].item())] += 1\n",
    "                    if text.startswith(\"<s>cultural heritage: \"):\n",
    "                        fail_count[\"false-positive\"][\"ch-\" + str(pred_labels[i].item())] += 1\n",
    "                        fail_count[\"false-negative\"][\"ch-\" + str(labels[i].item())] += 1\n",
    "                    if text.startswith(\"<s>history: \"):\n",
    "                        fail_count[\"false-positive\"][\"hist-\" + str(pred_labels[i].item())] += 1\n",
    "                        fail_count[\"false-negative\"][\"hist-\" + str(labels[i].item())] += 1\n",
    "                    if text.startswith(\"<s>people: \"):\n",
    "                        fail_count[\"false-positive\"][\"ppl-\" + str(pred_labels[i].item())] += 1\n",
    "                        fail_count[\"false-negative\"][\"ppl-\" + str(labels[i].item())] += 1\n",
    "                    if text.startswith(\"<s>management: \"):\n",
    "                        fail_count[\"false-positive\"][\"mgt-\" + str(pred_labels[i].item())] += 1\n",
    "                        fail_count[\"false-negative\"][\"mgt-\" + str(labels[i].item())] += 1\n",
    "                    if text.startswith(\"<s>nature: \"):\n",
    "                        fail_count[\"false-positive\"][\"nat-\" + str(pred_labels[i].item())] += 1\n",
    "                        fail_count[\"false-negative\"][\"nat-\" + str(labels[i].item())] += 1\n",
    "                    print(text)\n",
    "                    print(\"pred: \", pred_labels[i])\n",
    "                    print(\"labels: \", labels[i])\n",
    "                    print()\n",
    "    recalls = []; precisions = []; f1s = []\n",
    "    # recall, precision, f1\n",
    "    for key in type_counts:\n",
    "        if type_counts[key] > 0:\n",
    "            true_positive = type_counts[key] - fail_count[\"false-negative\"][key]\n",
    "            recall = 0; precision = 0; f1 = 0\n",
    "            if true_positive != 0:\n",
    "                recall = true_positive/type_counts[key]\n",
    "                precision = true_positive/(true_positive + fail_count[\"false-positive\"][key])\n",
    "                f1 = 2*recall*precision/(recall + precision)\n",
    "            print(key, \"recall=\", round(recall, 4), \"precision=\", round(precision, 4), \"f1=\", round(f1, 4))\n",
    "            recalls.append(recall); precisions.append(precision); f1s.append(f1)\n",
    "    # Accuracy\n",
    "    print(total_acc, dataloader.dataset.num_rows)\n",
    "    print(\"Accuracy:\", round(total_acc / dataloader.dataset.num_rows, 5))\n",
    "    # Loss\n",
    "    print(\"Loss:\", total_loss / len(dataloader))\n",
    "    # Marco recall, precision, f1\n",
    "    print(\"Marco-recall:\", round(sum(recalls)/len(recalls), 5))\n",
    "    print(\"Marco-precision:\", round(sum(precisions)/len(precisions), 5))\n",
    "    print(\"Marco-f1:\", round(sum(f1s)/len(f1s), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, raw_dataset, device, name='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
