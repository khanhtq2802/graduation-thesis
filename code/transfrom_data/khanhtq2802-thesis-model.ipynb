{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, DataCollatorWithPadding, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/khanh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_0 = AutoTokenizer.from_pretrained(\"khanhtq2802/thesis-model\")\n",
    "config_0 = AutoConfig.from_pretrained(\"khanhtq2802/thesis-model\")\n",
    "model_0 = AutoModelForSequenceClassification.from_pretrained(\"khanhtq2802/thesis-model\", force_download=False)\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(\"VietAI/envit5-translation\")\n",
    "config_1 = AutoConfig.from_pretrained(\"VietAI/envit5-translation\")\n",
    "model_1 = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/envit5-translation\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_0.to(device)\n",
    "model_1.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seoul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "with open(\"../data/seoul.csv\", 'r') as csvfile:\n",
    "    # skip the head\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader, None)\n",
    "    # write file\n",
    "    with open(\"../data/seoul-after-khanhtq2802-thesis-model.csv\", 'w') as csvfile1:\n",
    "        fieldnames = ['en_text', 'vn_text', 'confidence', 'bot_labels']\n",
    "        writer = csv.DictWriter(csvfile1, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in csv_reader:\n",
    "            for sentence in nltk.sent_tokenize(row[0]):\n",
    "                try:\n",
    "                    # get en_text\n",
    "                    en_text = 'en: ' + sentence\n",
    "                    # get vn_text\n",
    "                    inputs = [en_text]\n",
    "                    outputs = model_1.generate(tokenizer_1(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=150)\n",
    "                    vn_text = tokenizer_1.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "                    # get bot_labels and confidence\n",
    "                    aspects = ['', 'amn-', 'ch-', 'ppl-', 'mgt-', 'nat-', 'prc-']\n",
    "                    aspects_full = ['generality: ', 'amenities: ', 'cultural heritage: ', 'people: ', 'management: ', 'nature: ', 'price: ']\n",
    "                    inputs = []\n",
    "                    for i in aspects_full:\n",
    "                        inputs.append(i + sentence)\n",
    "\n",
    "                    encoded_inputs = tokenizer_0(inputs, return_tensors='pt', padding=True)\n",
    "                    encoded_inputs.to(device)\n",
    "                    outputs = model_0(**encoded_inputs)\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                    softmax = nn.Softmax(dim=1)(outputs)\n",
    "                    pred_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    bot_labels = str(pred_labels[0].item())\n",
    "                    confidence = softmax[0][pred_labels[0].item()]\n",
    "\n",
    "                    for i in range(1, len(pred_labels)):\n",
    "                        confidence += softmax[i][pred_labels[i].item()]\n",
    "                        if pred_labels[i] != 0:\n",
    "                            bot_labels += ' ' + aspects[i] + str(pred_labels[i].item())\n",
    "                    confidence = str(confidence.item() / len(pred_labels))\n",
    "\n",
    "                    writer.writerow({\n",
    "                    'en_text': en_text,\n",
    "                    'vn_text': vn_text,\n",
    "                    'confidence': confidence,\n",
    "                    'bot_labels': bot_labels,\n",
    "                    })\n",
    "                except Exception:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beijing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "with open(\"../data/beijing.csv\", 'r') as csvfile:\n",
    "    # skip the head\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader, None)\n",
    "    # write file\n",
    "    with open(\"../data/beijing-after-khanhtq2802-thesis-model.csv\", 'w') as csvfile1:\n",
    "        fieldnames = ['en_text', 'vn_text', 'confidence', 'bot_labels']\n",
    "        writer = csv.DictWriter(csvfile1, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in csv_reader:\n",
    "            for sentence in nltk.sent_tokenize(row[0]):\n",
    "                try:\n",
    "                    # get en_text\n",
    "                    en_text = 'en: ' + sentence\n",
    "                    # get vn_text\n",
    "                    inputs = [en_text]\n",
    "                    outputs = model_1.generate(tokenizer_1(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=150)\n",
    "                    vn_text = tokenizer_1.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "                    # get bot_labels and confidence\n",
    "                    aspects = ['', 'amn-', 'ch-', 'ppl-', 'mgt-', 'nat-', 'prc-']\n",
    "                    aspects_full = ['generality: ', 'amenities: ', 'cultural heritage: ', 'people: ', 'management: ', 'nature: ', 'price: ']\n",
    "                    inputs = []\n",
    "                    for i in aspects_full:\n",
    "                        inputs.append(i + sentence)\n",
    "\n",
    "                    encoded_inputs = tokenizer_0(inputs, return_tensors='pt', padding=True)\n",
    "                    encoded_inputs.to(device)\n",
    "                    outputs = model_0(**encoded_inputs)\n",
    "                    outputs = outputs.logits\n",
    "\n",
    "                    softmax = nn.Softmax(dim=1)(outputs)\n",
    "                    pred_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    bot_labels = str(pred_labels[0].item())\n",
    "                    confidence = softmax[0][pred_labels[0].item()]\n",
    "\n",
    "                    for i in range(1, len(pred_labels)):\n",
    "                        confidence += softmax[i][pred_labels[i].item()]\n",
    "                        if pred_labels[i] != 0:\n",
    "                            bot_labels += ' ' + aspects[i] + str(pred_labels[i].item())\n",
    "                    confidence = str(confidence.item() / len(pred_labels))\n",
    "\n",
    "                    writer.writerow({\n",
    "                    'en_text': en_text,\n",
    "                    'vn_text': vn_text,\n",
    "                    'confidence': confidence,\n",
    "                    'bot_labels': bot_labels,\n",
    "                    })\n",
    "                except Exception:\n",
    "                    continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
